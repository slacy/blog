<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="slacy" />

        <meta name="twitter:creator" content="@sklacy">
        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Python, TensorFlow, Jupyter, TensorFlow From The Ground Up, " />

<meta property="og:title" content="Nonlinear Units "/>
<meta property="og:url" content="./nonlinear-units.html" />
<meta property="og:description" content="This post part of my series TensorFlow From The Ground Up. In my previous post Learning to Add, I showed how a Neural Network architecture can learn a simple linear function of two variables. In this post, we’ll explore how to take our functions from linear to nonlinear. Additionally …" />
<meta property="og:site_name" content="Slacy&#39;s Blog" />
<meta property="og:article:author" content="slacy" />
<meta property="og:article:published_time" content="2017-02-10T11:00:00-08:00" />
<meta name="twitter:title" content="Nonlinear Units ">
<meta name="twitter:description" content="This post part of my series TensorFlow From The Ground Up. In my previous post Learning to Add, I showed how a Neural Network architecture can learn a simple linear function of two variables. In this post, we’ll explore how to take our functions from linear to nonlinear. Additionally …">

        <title>Nonlinear Units  · Slacy&#39;s Blog
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="./theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="./theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="./theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="./theme/css/admonition.css" media="screen">
        <link rel="stylesheet" type="text/css" href="./theme/css/custom.css" media="screen">
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
     })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-85612-4', 'auto');
    ga('send', 'pageview');
</script>



    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container-fluid">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="./"><span class=site-name>Slacy's Blog</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href=".">Home</a></li>
                            <li ><a href="./categories">Categories</a></li>
                            <li ><a href="./tags">Tags</a></li>
                            <li ><a href="./archives">Archives</a></li>
                            <li><form class="navbar-search" action="./search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
    <h1><a href="./nonlinear-units.html"> Nonlinear&nbsp;Units  </a></h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">

            
            <p>This post part of my series <a href="./category/tensorflow-from-the-ground-up.html">TensorFlow From The Ground Up</a>.</p>
<p>In my previous post <a href="./learning-to-add.html">Learning to Add</a>, I showed how a Neural Network architecture can learn a simple linear function of two&nbsp;variables.  </p>
<p>In this post, we&#8217;ll explore how to take our functions from linear to nonlinear.  Additionally, we&#8217;ll work on going some <strong>regressions</strong> and see what happens when we try to learn nonlinear&nbsp;functions. </p>
<h2>Composing linear&nbsp;functions</h2>
<p>One of the key points of linear functions is that the composition of two linear functions is also a linear function.  The math behind this is fairly&nbsp;straightforward: </p>
<div class="math">$$\begin{array}{rl}
f(x) &amp; = wx+b \\
f(f(x)) &amp; = w_2(w_1x+b_1)+b_2 \\
&amp; = (w_2w_1)x+(w_2b_1+b_2) \\
\end{array}$$</div>
<p>As you can see, the composed form of <span class="math">\(f(f(x))\)</span> is also a linear function, just with <span class="math">\(w = w_2w_1\)</span> and <span class="math">\(b =&nbsp;w_2b_1+b_2\)</span></p>
<p>Or, to think of this differently:  Without any <strong>nonlinear elements</strong> in our neural network architecture, there&#8217;s no reason to have any more than one node in the middle layer, because even a giant collection of linear functions still result in a single linear&nbsp;transformation.  </p>
<p>An exception to this is functions of multiple variables, like <span class="math">\(f(x,y) = \{a,b\}\)</span>  We can work the math backwards, but let it suffice to say that for functions of <span class="math">\(N\)</span> inputs and <span class="math">\(M\)</span> outputs, we need no more than <span class="math">\(\max(N,M)\)</span> linear 
nodes in the middle&nbsp;layer. </p>
<h2>How do we introduce&nbsp;nonlinearity?</h2>
<p>All we need to do is apply some elementwise nonlinear function to all the values in the middle layer of our network.  It ends up making our computation graph look like&nbsp;this: </p>
<div class="math">$$\begin{array}{rl}
middle\ layer &amp; = input \times W_{mid} + B_{mid} \\
middle\ nonlinear &amp; = f(middle\ layer) \\ 
output &amp; = (midle\ nonlinear) \times W_{out} + B_{out}
\end{array}$$</div>
<p>It&#8217;s that easy, we just need to choose the right nonlinear function to insert there.  Thankfully, this has been heavily researched, and there are 3 very commonly used&nbsp;functions:</p>
<ul>
<li><strong>sigmoid</strong> : <code>tf.nn.sigmoid</code></li>
<li><strong>tanh</strong> : <code>tf.nn.tanh</code></li>
<li><strong>ReLu</strong>  : <code>tf.nn.relu</code></li>
</ul>
<p>The best way to get a grasp on these is to look at them visually, so here we&nbsp;go: </p>
<div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="o">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s1">&#39;svg&#39;</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span> 

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span> 
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span> <span class="k">return</span> <span class="mi">0</span> 

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;tanh&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="n">relu</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;ReLU&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="svg" src="./images/NonlinearUnits_files/NonlinearUnits_5_0.svg"></p>
<h2>Simple&nbsp;Example</h2>
<p>As an example of computation that requires nonlinear units, I&#8217;d like to propose that we implement a function like&nbsp;this: 
</p>
<div class="math">$$f(x) = \begin{cases}
    1.0, &amp; \text{if } \lfloor x \rfloor = 42 \\
    0,         &amp; \text{otherwise}
\end{cases}
$$</div>
<p>This function isn&#8217;t easily expressed through any combination of linear operators.  Additionally, the fact that the output should be either 1.0 or 0.0 after training suggests that we should use the <strong>sigmoid</strong> function for activation of the neuron(s) in the&nbsp;network. </p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span> 
<span class="kn">import</span> <span class="nn">random</span>

<span class="n">t</span> <span class="o">=</span> <span class="mf">30.</span>
<span class="n">min_t</span> <span class="o">=</span> <span class="mf">25.</span>
<span class="n">max_t</span> <span class="o">=</span> <span class="mf">35.</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> 
    <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="n">t</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span> 
    <span class="k">return</span> <span class="mi">0</span>

<span class="n">i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input&#39;</span><span class="p">)</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;output&#39;</span><span class="p">)</span>

<span class="n">num_hidden</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">i_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">expected_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">),</span> 
                <span class="n">initial_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">),</span> <span class="n">mean</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">i_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">expected_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">),</span> 
                <span class="n">initial_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">),</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">mid</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i_w</span><span class="p">),</span> <span class="n">i_b</span><span class="p">))</span>
<span class="c1"># mid = tf.add(tf.matmul(i, i_w), i_b)</span>

<span class="n">o_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
    <span class="n">expected_shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
    <span class="n">initial_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">((</span><span class="n">num_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">mean</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">o_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
    <span class="n">expected_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
    <span class="n">initial_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">mid</span><span class="p">,</span> <span class="n">o_w</span><span class="p">),</span> <span class="n">o_b</span><span class="p">))</span>
<span class="c1"># output = tf.add(tf.matmul(mid, o_w), o_b)</span>

<span class="c1"># Our error function is computed as &quot;Mean Squared Difference&quot; between the computed output</span>
<span class="c1"># and the expected value. </span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">output</span> <span class="o">-</span> <span class="n">o</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">mean_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="c1"># Learning rate and optimizer similar to our previous examples. </span>
<span class="c1"># learning_rate = 0.0001</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">()</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span> 
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">local_variables_initializer</span><span class="p">(),</span> 
              <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()])</span>

    <span class="k">print</span> <span class="s2">&quot;TRAIN&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">200</span>
    <span class="n">train_iterations</span> <span class="o">=</span> <span class="mi">50000</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">train_iterations</span><span class="p">):</span>
        <span class="n">feed</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:[],</span> <span class="n">o</span><span class="p">:[]}</span>
        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">batch_size</span><span class="o">/</span><span class="mi">2</span><span class="p">):</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">min_t</span><span class="p">,</span> <span class="n">max_t</span><span class="p">)</span>
            <span class="n">feed</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">v</span><span class="p">,])</span>
            <span class="n">feed</span><span class="p">[</span><span class="n">o</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">f</span><span class="p">(</span><span class="n">v</span><span class="p">),])</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mean_loss</span><span class="p">,</span> <span class="n">output</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">5000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="s2">&quot;step =&quot;</span><span class="p">,</span><span class="n">step</span><span class="p">,</span><span class="s2">&quot; loss=&quot;</span><span class="p">,</span> <span class="n">l</span>

    <span class="k">print</span> <span class="s2">&quot;VALIDATE&quot;</span>
    <span class="n">validate_iterations</span> <span class="o">=</span> <span class="mi">15</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">validate_iterations</span><span class="p">):</span>
        <span class="n">feed</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:[],</span> <span class="n">o</span><span class="p">:[]}</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">min_t</span><span class="p">,</span> <span class="n">max_t</span><span class="p">)</span>
        <span class="n">feed</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">v</span><span class="p">,])</span>
        <span class="n">feed</span><span class="p">[</span><span class="n">o</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">f</span><span class="p">(</span><span class="n">v</span><span class="p">),])</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mean_loss</span><span class="p">,</span> <span class="n">output</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed</span><span class="p">)</span>
        <span class="k">print</span> <span class="s2">&quot;in = &quot;</span><span class="p">,</span> <span class="n">feed</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="s2">&quot;output =&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="s2">&quot;expected = &quot;</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="s2">&quot; loss=&quot;</span><span class="p">,</span> <span class="n">l</span>


    <span class="k">print</span> <span class="s2">&quot;GRAPH&quot;</span>
    <span class="n">tx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">min_t</span><span class="p">,</span><span class="n">max_t</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">i</span><span class="p">:[[</span><span class="n">ix</span><span class="p">,]</span> <span class="k">for</span> <span class="n">ix</span> <span class="ow">in</span> <span class="n">tx</span><span class="p">],})</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tx</span><span class="p">,</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">ys</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;learned&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tx</span><span class="p">,</span> <span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">fx</span><span class="p">)</span> <span class="k">for</span> <span class="n">fx</span> <span class="ow">in</span> <span class="n">tx</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>TRAIN
step = 0  loss= 1.83179e+06
step = 5000  loss= 0.156334
step = 10000  loss= 0.197727
step = 15000  loss= 0.145691
step = 20000  loss= 0.0489364
step = 25000  loss= 0.0920705
step = 30000  loss= 0.0996958
step = 35000  loss= 0.0495032
step = 40000  loss= 0.0642934
step = 45000  loss= 0.0641886
VALIDATE
in =  [[32.003762118671474]] output = [[ 0.76464736]] expected =  1  loss= 0.0553909
in =  [[32.36696377797531]] output = [[ 0.82751727]] expected =  1  loss= 0.0297503
in =  [[29.104202468396544]] output = [[ 0.24386668]] expected =  0  loss= 0.059471
in =  [[25.386498517093727]] output = [[ 0.]] expected =  0  loss= 0.0
in =  [[33.06359502062598]] output = [[ 0.94786686]] expected =  1  loss= 0.00271786
in =  [[25.46369592630006]] output = [[ 0.]] expected =  0  loss= 0.0
in =  [[34.76348901877117]] output = [[ 1.2494638]] expected =  1  loss= 0.0622322
in =  [[32.77071202248998]] output = [[ 0.88596189]] expected =  1  loss= 0.0130047
in =  [[33.86705852782782]] output = [[ 1.07837367]] expected =  1  loss= 0.00614243
in =  [[33.21695256720534]] output = [[ 0.95710593]] expected =  1  loss= 0.0018399
in =  [[25.06452496846031]] output = [[ 0.]] expected =  0  loss= 0.0
in =  [[26.761287670627226]] output = [[ 0.]] expected =  0  loss= 0.0
in =  [[31.26943022484645]] output = [[ 0.60015279]] expected =  1  loss= 0.159878
in =  [[34.61701252905589]] output = [[ 1.20263445]] expected =  1  loss= 0.0410607
in =  [[31.322623992257483]] output = [[ 0.61499643]] expected =  1  loss= 0.148228
GRAPH
</pre></div>


<p><img alt="svg" src="./images/NonlinearUnits_files/NonlinearUnits_7_1.svg"></p>
<h2>You should play with this code a little&nbsp;bit.</h2>
<p>Here&#8217;s a collection of random ideas for how to play around with the code example above and gain some&nbsp;insights:</p>
<ul>
<li>Modify the &#8220;f()&#8221; function to try other linear combinations of x <span class="amp">&amp;</span> y.  Can it learn <span class="math">\(x-y\)</span>?  Can it learn <span class="math">\(0.5x + 0.75y - 0.33\)</span>? </li>
<li>Modify the size of the middle layer.  We use 2 middle layer nodes.  What if you use 200?  How does that impact learning&nbsp;rate?   </li>
<li>Modify the size of the middle layer, and have it try to learn something &#8220;Hard&#8221; like <span class="math">\(x\cdot y\)</span>. Did it work?  Do you have any thoughts about why or why&nbsp;not?</li>
</ul>
<div class="highlight"><pre><span></span>
</pre></div>


<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            <div>
</div>

            
            
            <hr/>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2017-02-10T11:00:00-08:00">Feb 10, 2017</time>
            <h4>Category</h4>
            <a class="category-link" href="./categories.html#tensorflow-from-the-ground-up-ref">TensorFlow From The Ground Up</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="./tags#jupyter-ref">Jupyter
                    <span>6</span>
</a></li>
                <li><a href="./tags#python-ref">Python
                    <span>6</span>
</a></li>
                <li><a href="./tags#tensorflow-ref">TensorFlow
                    <span>6</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="https://twitter.com/sklacy" title="My Twitter @sklacy Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-twitter @sklacy sidebar-social-links"></i></a>
    <a href="https://github.com/slacy" title="My Github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="https://github.com/Pelican-Elegant/elegant/" title="Theme Elegant Home Page">Elegant</a></li>
    </ul>
</div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    
    </body>
    <!-- Theme: Elegant built for Pelican
    License : MIT -->
</html>