Title: Parallel sorting via the linux commandline
Date: 2008-06-11 22:35
Author: slacy
Category: General
Status: published

In my [previous
post](http://slacy.com/blog/index.php/2008/06/10/unix-commandline-challenge/),
I challenged you to write a shell script that could grep a large file in
parallel.

Several comments on that post mentioned that grep was very fast, and
that I/O is clearly the bottleneck, so parallelizing will be of very
little help.

With that in mind, I now challenge you to write a simple script to
parallelize sorting of a single large file. The file is generated by the
same [randwords.pl](http://slacy.com/blog/wp-content/randwords.pl)
script, but this time, its:

    # LC_ALL=C wc randwords 
      92995543   92995544 1069481984 randwords

92M lines, just over 1020MB of random junk.

Since the perl "sharding" script didn't work out too well, and since
I've learned to embrace mkfifo, and I've written a simple C++ helper
program I call 'interleave'. The source is:

    // interleave.cc
    #include <stdio.h>
    #include <stdlib.h>
    int main(int argc, char *argv[]) {
      int num_files = argc - 1; 
      FILE *outs[num_files];
      for (int i = 0; i < argc; i++) {
        outs[i] = fopen(argv[i + 1], "a");
      }
      // Credits go to 'man getline' for the guts of this...
      ssize_t read;
      char *line = NULL;
      size_t len = 0;
      int counter = 0; 
      while ((read = getline(&line, &len, stdin)) != -1) {
        // I'm pretty sure that buffering here would make a huge difference in performance.
        fwrite(line, read, 1, outs[counter % num_files]);
        counter++;
      }
      if (line)
        free(line);
      for (int i =0; i < num_files; i++) {
        fclose(outs[i]); 
      }
      return EXIT_SUCCESS;
    }

So, this will take an input file, and "interleave" the lines of the file
across an arbitrary set of output files. Its very similar to the
shard.pl, except that its C, so its moderately fast. (NB: Still lots of
room for improvement here, but it gets the job done.)

Once that was tested and working, I wrote a small wrapper script to
create some fifos, and spawn off a bunch of sort processes.

    #!/bin/bash
    # psort.sh 
    NWAY=4
    COUNTER=0
    until [ $COUNTER -ge $NWAY ]; do
    mkfifo $1_chunk_$COUNTER;
    COUNTER=`expr $COUNTER + 1`
    done
    echo "interleaving" 
    cat $1 | ./interleave $1_chunk_* &
    for file in $1_chunk_*; do
    echo "sorting $file in the background"
    sort $file > sorted.$file &
    done
    # Wait for backgrounded interleave & sorts to finish up. 
    wait 
    echo "Merging" 
    sort -m sorted.$1_chunk_*  > result 
    rm $1_chunk_* sorted.$1_chunk_*

I've called this 'psort.sh'. All we're doing there is spawing an
interleave in the background, outputting to a series of named pipes, and
then sorting each of those pipes and storing them as a temporary file.
We then merge all the temporary files together to form the final results
file.

How does it perform? Well, about 12.5% faster than a naive invocation of
'sort'. Here's the proof:

    # time ./psort.sh randwords 
    Making fifo randwords_chunk_0;
    Making fifo randwords_chunk_1;
    Making fifo randwords_chunk_2;
    Making fifo randwords_chunk_3;
    splitting
    sorting randwords_chunk_0 in the background
    sorting randwords_chunk_1 in the background
    sorting randwords_chunk_2 in the background
    sorting randwords_chunk_3 in the background
    Merging

    real    3m34.493s
    user    3m36.564s
    sys     0m32.885s

    # time sort randwords > results.1proc
    real    4m0.802s
    user    3m27.967s
    sys     0m14.958s

So, why only 12.5% faster? Well, 2 reasons that I can think of:

1\. "interleave" was taking up about 20% of the CPU for the psort
invocation, and I believe it could be significantly optimized. I tried
invoking 'split' in interleave.sh, and splitting out to a bunch of
fifos, but because of the linear nature of that, it didn't have the same
level of parallelism that the interleave command enables.

2\. The temporary sort includes 2 merge phases. First, each of the
parallel sorts has to merge its temporary files (in parallel!), and this
is extremely I/O intensive, and then the "sort -m" has to merge those
outputs, so there are 2 phases of merging, which takes away
significantly from the overall runtime.

If you'd like to help optimize interleave.cc, or to come up with a
better way to do this, feel free to leave a comment with your solution!
