{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Title: Feature Engineering (Counting the Bits)\n",
    "Category: TensorFlow From The Ground Up\n",
    "Tags: Python, TensorFlow, Jupyter\n",
    "Date: 3/13/2017 4:30pm\n",
    "Author: slacy\n",
    "Status: draft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is the task of deciding how to represent the data that goes in and out of your Machine Learning system.  Feature Engineering is a vital part of network design, because it turns out that how features (i.e. data) is represented has a huge impact on how easy it will be for your system to learn the task that you are interested in.\n",
    "\n",
    "To illustrate how important Feature Engineering is, one good approach is to \"make up\" some synthetic examples with synthetic inputs & outputs, and see if we can learn the task at hand. \n",
    "\n",
    "## Counting Bits in Binary \n",
    "\n",
    "For this exercise, we are going to use \"counting the binary bits\" (also referred to as \"*popcount*\" or \"*Hamming Distance*\") as the function we are trying to learn.  This function cannot be represented by a linear function, se we'll be using a 2-layer network.  We will constrain our input our values in the range $[0,1024)$, and our outputs will be in the range $[0,10]$.  We can think of our training set as looking somewhat like this:\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline input & popcount(input) \\\\\\hline\n",
    "  374 & 5 \\\\\\hline\n",
    "  924 & 6 \\\\\\hline\n",
    "  708 & 4 \\\\\\hline\n",
    "  6 & 2 \\\\\\hline\n",
    "  ... & ... \\\\\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Let's try passing these integer values directly into the first layer of the network and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "i= 0  loss= 4.62908\n",
      "i= 1000  loss= 0.419672\n",
      "i= 2000  loss= 0.350638\n",
      "i= 3000  loss= 0.413125\n",
      "i= 4000  loss= 0.352155\n",
      "i= 5000  loss= 0.327442\n",
      "i= 6000  loss= 0.344848\n",
      "i= 7000  loss= 0.356394\n",
      "i= 8000  loss= 0.305952\n",
      "i= 9000  loss= 0.308031\n",
      "i= 10000  loss= 0.283118\n",
      "i= 11000  loss= 0.369738\n",
      "i= 12000  loss= 0.303562\n",
      "i= 13000  loss= 0.308296\n",
      "i= 14000  loss= 0.333424\n",
      "i= 15000  loss= 0.30559\n",
      "i= 16000  loss= 0.291579\n",
      "i= 17000  loss= 0.351013\n",
      "i= 18000  loss= 0.338948\n",
      "i= 19000  loss= 0.362444\n",
      "i= 20000  loss= 0.304563\n",
      "i= 21000  loss= 0.180726\n",
      "i= 22000  loss= 0.266295\n",
      "i= 23000  loss= 0.270902\n",
      "i= 24000  loss= 0.25015\n",
      "i= 25000  loss= 0.241592\n",
      "i= 26000  loss= 0.272542\n",
      "i= 27000  loss= 0.272879\n",
      "i= 28000  loss= 0.263778\n",
      "i= 29000  loss= 0.211322\n",
      "i= 30000  loss= 0.282963\n",
      "i= 31000  loss= 0.319068\n",
      "i= 32000  loss= 0.230075\n",
      "i= 33000  loss= 0.284408\n",
      "i= 34000  loss= 0.301685\n",
      "i= 35000  loss= 0.247171\n",
      "i= 36000  loss= 0.319898\n",
      "i= 37000  loss= 0.253544\n",
      "i= 38000  loss= 0.240501\n",
      "i= 39000  loss= 0.2356\n",
      "i= 40000  loss= 0.280686\n",
      "i= 41000  loss= 0.226019\n",
      "i= 42000  loss= 0.214324\n",
      "i= 43000  loss= 0.262179\n",
      "i= 44000  loss= 0.198153\n",
      "i= 45000  loss= 0.261084\n",
      "i= 46000  loss= 0.240503\n",
      "i= 47000  loss= 0.235443\n",
      "i= 48000  loss= 0.269355\n",
      "i= 49000  loss= 0.314097\n",
      "i= 50000  loss= 0.238789\n",
      "i= 51000  loss= 0.203914\n",
      "i= 52000  loss= 0.29305\n",
      "i= 53000  loss= 0.291942\n",
      "i= 54000  loss= 0.302206\n",
      "i= 55000  loss= 0.242575\n",
      "i= 56000  loss= 0.27227\n",
      "i= 57000  loss= 0.223717\n",
      "i= 58000  loss= 0.249152\n",
      "i= 59000  loss= 0.202866\n",
      "i= 60000  loss= 0.208728\n",
      "i= 61000  loss= 0.235625\n",
      "i= 62000  loss= 0.203516\n",
      "i= 63000  loss= 0.245659\n",
      "i= 64000  loss= 0.253923\n",
      "i= 65000  loss= 0.213467\n",
      "i= 66000  loss= 0.239217\n",
      "i= 67000  loss= 0.231245\n",
      "i= 68000  loss= 0.256408\n",
      "i= 69000  loss= 0.231592\n",
      "i= 70000  loss= 0.197811\n",
      "i= 71000  loss= 0.231928\n",
      "i= 72000  loss= 0.254814\n",
      "i= 73000  loss= 0.227593\n",
      "i= 74000  loss= 0.217913\n",
      "i= 75000  loss= 0.254691\n",
      "i= 76000  loss= 0.285135\n",
      "i= 77000  loss= 0.235177\n",
      "i= 78000  loss= 0.205863\n",
      "i= 79000  loss= 0.21152\n",
      "i= 80000  loss= 0.244499\n",
      "i= 81000  loss= 0.227693\n",
      "i= 82000  loss= 0.229226\n",
      "i= 83000  loss= 0.219194\n",
      "i= 84000  loss= 0.232463\n",
      "i= 85000  loss= 0.210225\n",
      "i= 86000  loss= 0.246284\n",
      "i= 87000  loss= 0.222384\n",
      "i= 88000  loss= 0.234835\n",
      "i= 89000  loss= 0.214603\n",
      "i= 90000  loss= 0.21836\n",
      "i= 91000  loss= 0.241133\n",
      "i= 92000  loss= 0.222177\n",
      "i= 93000  loss= 0.233147\n",
      "i= 94000  loss= 0.238967\n",
      "i= 95000  loss= 0.200362\n",
      "i= 96000  loss= 0.215999\n",
      "i= 97000  loss= 0.193563\n",
      "i= 98000  loss= 0.207447\n",
      "i= 99000  loss= 0.230364\n",
      "VALIDATE\n",
      "input= [[7]]  out= [[ 2.09152031]] actual=, [[3]]  loss= 0.825335\n",
      "input= [[3]]  out= [[ 1.37256026]] actual=, [[2]]  loss= 0.393681\n",
      "input= [[5]]  out= [[ 1.95039511]] actual=, [[2]]  loss= 0.00246065\n",
      "input= [[11]]  out= [[ 2.25213575]] actual=, [[3]]  loss= 0.559301\n",
      "input= [[10]]  out= [[ 1.9918443]] actual=, [[2]]  loss= 6.65155e-05\n",
      "input= [[3]]  out= [[ 1.37256026]] actual=, [[2]]  loss= 0.393681\n",
      "input= [[10]]  out= [[ 1.9918443]] actual=, [[2]]  loss= 6.65155e-05\n",
      "input= [[9]]  out= [[ 1.8509928]] actual=, [[2]]  loss= 0.0222031\n",
      "input= [[14]]  out= [[ 3.30856967]] actual=, [[3]]  loss= 0.0952152\n",
      "input= [[11]]  out= [[ 2.25213575]] actual=, [[3]]  loss= 0.559301\n",
      "input= [[1]]  out= [[ 0.97653669]] actual=, [[1]]  loss= 0.000550527\n",
      "input= [[12]]  out= [[ 2.57091808]] actual=, [[2]]  loss= 0.325947\n",
      "input= [[8]]  out= [[ 1.90836024]] actual=, [[1]]  loss= 0.825118\n",
      "input= [[3]]  out= [[ 1.37256026]] actual=, [[2]]  loss= 0.393681\n",
      "input= [[3]]  out= [[ 1.37256026]] actual=, [[2]]  loss= 0.393681\n",
      "input= [[10]]  out= [[ 1.9918443]] actual=, [[2]]  loss= 6.65155e-05\n",
      "input= [[6]]  out= [[ 2.13547993]] actual=, [[2]]  loss= 0.0183548\n",
      "input= [[11]]  out= [[ 2.25213575]] actual=, [[3]]  loss= 0.559301\n",
      "input= [[12]]  out= [[ 2.57091808]] actual=, [[2]]  loss= 0.325947\n",
      "input= [[3]]  out= [[ 1.37256026]] actual=, [[2]]  loss= 0.393681\n",
      "input= [[0]]  out= [[ 0.00694078]] actual=, [[0]]  loss= 4.81745e-05\n",
      "input= [[12]]  out= [[ 2.57091808]] actual=, [[2]]  loss= 0.325947\n",
      "input= [[10]]  out= [[ 1.9918443]] actual=, [[2]]  loss= 6.65155e-05\n",
      "input= [[5]]  out= [[ 1.95039511]] actual=, [[2]]  loss= 0.00246065\n",
      "input= [[1]]  out= [[ 0.97653669]] actual=, [[1]]  loss= 0.000550527\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import random\n",
    "\n",
    "inputs = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='input')\n",
    "popcount = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='popcount')\n",
    "\n",
    "input_weight = tf.Variable(expected_shape=(1, 16), \n",
    "                           initial_value=tf.truncated_normal((1, 16), mean=0, stddev=0.1))\n",
    "input_bias = tf.Variable(expected_shape=(1,16), \n",
    "                         initial_value=tf.truncated_normal((1,16), mean=0, stddev=0.1))\n",
    "\n",
    "# \"Perceptron\" \n",
    "mid_layer = tf.nn.sigmoid(tf.add(tf.matmul(inputs, input_weight), input_bias))\n",
    "\n",
    "out_weight = tf.Variable(expected_shape=(16, 1), \n",
    "                         initial_value=tf.truncated_normal((16, 1), mean=0, stddev=0.1))\n",
    "out_bias = tf.Variable(expected_shape=(1, 1), \n",
    "                       initial_value=tf.truncated_normal((1, 1), mean=0, stddev=0.1))\n",
    "\n",
    "# Perceptron formula again.\n",
    "output = tf.add(tf.matmul(mid_layer, out_weight), out_bias)\n",
    "\n",
    "# Our error function is computed as \"Squared Difference\" between the computed output\n",
    "# and the expected value. \n",
    "loss = tf.reduce_mean(tf.pow(output - popcount, 2))\n",
    "\n",
    "# Learning rate and optimizer similar to our previous examples. \n",
    "learning_rate = 0.001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "def f(x): \n",
    "    popcount = 0 \n",
    "    while x: \n",
    "        if x&1:\n",
    "            popcount+=1\n",
    "        x>>=1 \n",
    "    return popcount\n",
    "\n",
    "def makeinput(x):\n",
    "    return [x,]\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run([tf.local_variables_initializer(), \n",
    "              tf.global_variables_initializer()])\n",
    "    \n",
    "    # Training: \n",
    "    #\n",
    "    # Iterate many times with random inputs to \"learn\" the parameters\n",
    "    # stored in input_wegiht, input_bias, out_weight, out_bias Variables above.  \n",
    "    # For this example, we only pass in **even** numbers in the range [0,20]\n",
    "    # \n",
    "    # We will use odd values during our validation phase below to ensure that \n",
    "    # we never validate on any of the inputs that were in the training set. \n",
    "    print \"TRAIN\"\n",
    "    train_iterations = 100000\n",
    "    batch_size = 128\n",
    "    max_value = 16\n",
    "    for it in xrange(train_iterations):\n",
    "        feed = {inputs:[], popcount:[]}\n",
    "        for bt in xrange(batch_size):\n",
    "            v = random.randrange(0, max_value)\n",
    "            feed[inputs].append(makeinput(v))\n",
    "            feed[popcount].append(makeinput(f(v)))\n",
    "        _, l = sess.run([optimizer, loss], feed_dict=feed)\n",
    "        if it % 1000 == 0:\n",
    "            print \"i=\",it,\" loss=\", l\n",
    "            \n",
    "    # Once we have learned the parameters, we can validate by passing inputs \n",
    "    # never seen before.  For this case, we expand the range of our inputs \n",
    "    # to include all odd numbers in the range [-40,40].  \n",
    "    print \"VALIDATE\"\n",
    "    validate_iterations = 25\n",
    "    for it in xrange(validate_iterations):\n",
    "        v = random.randrange(0, max_value)\n",
    "        i = [makeinput(v),]\n",
    "        e = [makeinput(f(v)),]\n",
    "        out, l = sess.run([output, loss], feed_dict={inputs:i, popcount:e})\n",
    "        print \"input=\",i,\" out=\", out, \"actual=,\", e,\" loss=\", l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
