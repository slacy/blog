{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Title: Learning to Multiply, Part 1\n",
    "Category: TensorFlow From The Ground Up\n",
    "Tags: Python, TensorFlow, Jupyter\n",
    "Date: 2/9/2017 1:00pm\n",
    "Author: slacy\n",
    "Status: draft\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the **fourth** post of my series [TensorFlow From The Ground Up]({category}tensorflow-from-the-ground-up).\n",
    "\n",
    "In my previous post, [Learning to Add]({filename}/Learning to Add.md) I demonstrated a toy neural network that was able to learn how to add two real-valued variables.  It learned a general formula that applied to *any* real-valued input values, even though we only trained it on values in the range [0,20].  (Note: This is *really* amazing!) \n",
    "\n",
    "Now, we're going to start to ask some more difficult questions:\n",
    "\n",
    "> How do we know when a network is able to learn a function, and when it isn't? \n",
    ">\n",
    "> What parameters can we tweak help a network to learn? \n",
    ">\n",
    "> Is it possible that some functions are \"unlearnable\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "i= 0  loss= [ 343.70489502]\n",
      "i= 1000  loss= [ 0.00222525]\n",
      "i= 2000  loss= [  9.52743212e-05]\n",
      "i= 3000  loss= [  7.60952462e-05]\n",
      "i= 4000  loss= [  2.87974472e-05]\n",
      "i= 5000  loss= [  2.28197314e-06]\n",
      "i= 6000  loss= [  6.43522071e-08]\n",
      "i= 7000  loss= [  3.78495315e-08]\n",
      "i= 8000  loss= [  1.07751148e-05]\n",
      "i= 9000  loss= [  5.25324140e-09]\n",
      "VALIDATE\n",
      "x= -15  y= 29  out= 14.0  loss= [  1.78260962e-10]\n",
      "x= -19  y= -11  out= -29.9999  loss= [  1.88592821e-08]\n",
      "x= 7  y= 9  out= 16.0  loss= [ 0.]\n",
      "x= 3  y= 9  out= 12.0  loss= [  9.09494702e-11]\n",
      "x= -1  y= -35  out= -35.9998  loss= [  2.94676283e-08]\n",
      "x= 21  y= -13  out= 8.00004  loss= [  1.60434865e-09]\n",
      "x= -39  y= 7  out= -31.9999  loss= [  1.63308869e-08]\n",
      "x= -3  y= 17  out= 14.0  loss= [  3.63797881e-12]\n",
      "x= 21  y= 29  out= 49.9999  loss= [  1.14087015e-08]\n",
      "x= -33  y= 3  out= -29.9999  loss= [  1.53704605e-08]\n",
      "[array([[-0.73145592,  0.41055757],\n",
      "       [-0.71037835,  0.44684216]], dtype=float32), array([[ 0.12508716,  0.19557403]], dtype=float32), array([[-1.03099132],\n",
      "       [ 0.59887499]], dtype=float32), array([[ 0.01188867]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import random\n",
    "\n",
    "# Here are the inputs to our computation Graph.  Note that the expected value \n",
    "# f(x,y)=x+y is passed a via a placeholder as well. \n",
    "x = tf.placeholder(shape=(1,), dtype=tf.float32, name='x')\n",
    "y = tf.placeholder(shape=(1,), dtype=tf.float32, name='y')\n",
    "expected = tf.placeholder(shape=(1,), dtype=tf.float32, name='expected')\n",
    "\n",
    "# Inputs need to be in the form of a single tensor, so we concatenate and reshape \n",
    "# into a form that we can use below. \n",
    "inputs = tf.reshape(tf.concat(0, [x,y]), (1,2))\n",
    "\n",
    "# We are constructing a middle layer of 2 perceptron nodes.  Weights and biases \n",
    "# are our w_i and v_i from the above equations. \n",
    "input_weight = tf.Variable(expected_shape=[2,2], \n",
    "                           initial_value=tf.truncated_normal([2,2], mean=0, stddev=0.1))\n",
    "input_bias = tf.Variable(expected_shape=[1, 2], \n",
    "                         initial_value=tf.truncated_normal([1, 2], mean=0, stddev=0.1))\n",
    "\n",
    "# Our middle layer of \"nodes\" (can be thought of as just an intermediate value)\n",
    "# Is an array of size (1,2) and each value is computed from our perceptron function: \n",
    "mid_layer = tf.add(tf.matmul(inputs, input_weight), input_bias)\n",
    "\n",
    "# We're looking for a single output value, so we apply the same perceptron formula \n",
    "# to our middle layer, but this time multiplying by a tensor of size (2,1) to produce \n",
    "# an output of size (1,1).  We then \"squeeze\" this value down to a tensor of size (1)\n",
    "out_weight = tf.Variable(expected_shape=[2,1], \n",
    "                         initial_value=tf.truncated_normal([2,1], mean=0, stddev=0.1))\n",
    "out_bias = tf.Variable(expected_shape=[1, 1], \n",
    "                       initial_value=tf.truncated_normal([1,1], mean=0, stddev=0.1))\n",
    "\n",
    "# Perceptron formula again, and the squeeze to get a single value. \n",
    "output = tf.squeeze(tf.add(tf.matmul(mid_layer, out_weight), out_bias))\n",
    "\n",
    "# Our error function is computed as \"Squared Difference\" between the computed output\n",
    "# and the expected value. \n",
    "loss = tf.pow(output - expected, 2)\n",
    "\n",
    "# Learning rate and optimizer similar to our previous examples. \n",
    "learning_rate = 0.001\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "def f(x,y): return x + y\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run([tf.local_variables_initializer(), \n",
    "              tf.global_variables_initializer()])\n",
    "    \n",
    "    # Training: \n",
    "    #\n",
    "    # Iterate many times with random inputs to \"learn\" the parameters\n",
    "    # stored in input_wegiht, input_bias, out_weight, out_bias Variables above.  \n",
    "    # For this example, we only pass in **even** numbers in the range [0,20]\n",
    "    # \n",
    "    # We will use odd values during our validation phase below to ensure that \n",
    "    # we never validate on any of the inputs that were in the training set. \n",
    "    print \"TRAIN\"\n",
    "    train_iterations = 10000\n",
    "    for i in xrange(train_iterations):\n",
    "        ix = [random.randrange(0, 10) * 2,]\n",
    "        iy = [random.randrange(0, 10) * 2,]\n",
    "        e = [f(ix[0],iy[0]),]\n",
    "        _, l = sess.run([optimizer, loss], feed_dict={x:ix, y:iy, expected:e})\n",
    "        if i % 1000 == 0:\n",
    "            print \"i=\",i,\" loss=\", l\n",
    "            \n",
    "    # Once we have learned the parameters, we can validate by passing inputs \n",
    "    # never seen before.  For this case, we expand the range of our inputs \n",
    "    # to include all odd numbers in the range [-40,40].  \n",
    "    print \"VALIDATE\"\n",
    "    validate_iterations = 10\n",
    "    for i in xrange(validate_iterations):\n",
    "        ix = [random.randrange(-20, 20) * 2 + 1,]\n",
    "        iy = [random.randrange(-20, 20) * 2 + 1,]\n",
    "        e = [f(ix[0], iy[0]),]\n",
    "        out, l = sess.run([output, loss], feed_dict={x:ix, y:iy, expected:e})\n",
    "        print \"x=\",ix[0], \" y=\",iy[0], \" out=\", out, \" loss=\", l\n",
    "    \n",
    "    # Print out the computed weights and bisaes for inspection. \n",
    "    print sess.run([input_weight, input_bias, out_weight, out_bias])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## You should play with this code a little bit.\n",
    "\n",
    "Here's a collection of random ideas for how to play around with the code example above and gain some insights:\n",
    "\n",
    "* Modify the \"f()\" function to try other linear combinations of x & y.  Can it learn $x-y$?  Can it learn $0.5x + 0.75y - 0.33$? \n",
    "* Modify the size of the middle layer.  We use 2 middle layer nodes.  What if you use 200?  How does that impact learning rate?   \n",
    "* Modify the size of the middle layer, and have it try to learn something \"Hard\" like $x\\cdot y$. Did it work?  Do you have any thoughts about why or why not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
