{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Title: Basics of TensorFlow\n",
    "Category: TensorFlow From The Ground Up\n",
    "Tags: Pelican, Python, TensorFlow, Jupyter\n",
    "Date: 1/31/2017 11:00pm \n",
    "Author: slacy\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I first heard about TensorFlow, it was described as a library for doing Machine Learning, which I equated with Neural Networks.  (Neural Networks are just one type of Machine Learning).  \n",
    "\n",
    "I had read many other blog posts about Neural Network architectures, so I assumed it would have an API that looked something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here's What I thought the TensorFlow API might look like, \n",
    "# before I had read any code or documentation about it.\n",
    "network = tf.Network() \n",
    "network = network.AddConvolutionalLayer(...)\n",
    "network = network.AddMaxPoolingLayer(...)\n",
    "network = network.AddSigmoidActivation(...)\n",
    "network.Train(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boy, was I wrong!  \n",
    "\n",
    "The core API of TensorFlow is much lower level than what I've shown above!\n",
    "\n",
    "TensorFlow can be thought of as being most similar to the Python library [SymPy](http://www.sympy.org/en/index.html) or a \n",
    "symbolic mathematics language like R, Matlab or Mathematica.    But, TensorFlow isn't a generic symbolic math package, it's a symbolic math library whose sole purpose is to calculate numeric approximations of functions.  In other words, you can think of it like a \"solver\" using an algorithm like [Newton's Method](https://en.wikipedia.org/wiki/Newton%27s_method) or [Runge-Kutta](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods), except that it's specifically focused on techniques used in ML: Stochastic Gradient Descent.  \n",
    "\n",
    "I know that sounds like a lot, and you really don't need to know what all that stuff means!   \n",
    "\n",
    "When using TensorFlow, what you're doing is setting up \n",
    "a [computation graph](https://www.tensorflow.org/get_started/basic_usage#the_computation_graph) for your function(s).  \n",
    "\n",
    "## Computing y=x^2 using TensorFlow\n",
    "\n",
    "Let's write a quick example TensorFlow program and see how the simple parts work.  Here, we're going to compute $y=x^2$ using TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value of x =  1.0\n",
      "Initial value of y=x^2 =  1.0\n",
      "5^2 =  25.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# TensorFlow Variables are for values used during computation.  \n",
    "x = tf.Variable(dtype=tf.float32, initial_value=1.0, name='x') \n",
    "\n",
    "# Even though this looks like native python math, it is not. \n",
    "# The type of 'x' is a TensorFlow variable, so \"x**2\" doesn't just \n",
    "# compute a value, it returns a computation graph that computes\n",
    "# \"x^2\" for whatever the current value of x is.  \n",
    "y = x**2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # There are internal global variables that always need to be initialized, \n",
    "    # as well as our own \"initial_value=1.0\" for 'x'.  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print \"Initial value of x = \", sess.run(x)\n",
    "\n",
    "    # Now, we can confirm that y=1^2 produces 1.0: \n",
    "    print \"Initial value of y=x^2 = \", sess.run(y)\n",
    "    \n",
    "    # We can't just say \"x=5\" because that would change the type of x from a \n",
    "    # tf.Variable into a native Python float.  We need to create & run a \n",
    "    # graph that reassigns the value of x.  \n",
    "    sess.run(x.assign(5))\n",
    "    \n",
    "    # Once we have reassigned x, we can recompute the 'y' graph. \n",
    "    print \"5^2 = \", sess.run(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there you have it.  TensorFlow can compute some values, which is not a huge surprise.  The syntax is a bit verbose, but the structure I've used above is very common, even in larger systems, so its useful to start out this way. \n",
    "\n",
    "On to bigger and better things.  How to we run a Solver?\n",
    "\n",
    "## Running a solver & finding a minimum\n",
    "\n",
    "Before we run a solver, we must create a computation graph whose result is a single value that we would like to **minimize**.  Finding values that minimize functions is what TensorFlow is really, really good at. \n",
    "\n",
    "Let's assume that what we want to compute is $\\min(x^2)$ so we restate this as \"Find me the value of '$x$' that minimizes the function $x^2$\" \n",
    " \n",
    "So, here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min x =  0.998\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(dtype=tf.float32, initial_value=1.0, name='x')\n",
    "y = tf.pow(x, 2)\n",
    "\n",
    "learning_rate = 0.001 \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(optimizer.minimize(y))\n",
    "    print \"min x = \", sess.run(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait, what happened?\n",
    "\n",
    "We know that the minimum of $x^2$ is at $x=0$ so what happened?  Where did 0.998 come from, and what's \"Learning Rate\"? \n",
    "\n",
    "Remember, TensorFlow uses GradientDescent, which is an iterative process.  The code above only ran *one iteration* of the algorithm, so only made a small change to the initial value of $x$.  The Good Thing is that it seems to be going in the right direction.  \n",
    "\n",
    "What it actually computed was:\n",
    "$$x_1=x_0 -(learning\\_rate * \\frac{\\partial x^2}{\\partial x})$$\n",
    "\n",
    "$learning\\_rate = 0.001$, \n",
    "the derivative of $x^2$ is $2x$, and \n",
    "the initial value of x was 1.0, that leaves us with:\n",
    "\n",
    "$$x_1=1.0-0.001*2x$$\n",
    "\n",
    "Which is exactly 0.998.  Whew! \n",
    "\n",
    "So, now we can rewrite our solver. \n",
    "\n",
    "## A solver that actually works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0  x= 0.998\n",
      "i= 1000  x= 0.134794\n",
      "i= 2000  x= 0.0182059\n",
      "i= 3000  x= 0.00245898\n",
      "i= 4000  x= 0.000332121\n",
      "i= 5000  x= 4.48577e-05\n",
      "i= 6000  x= 6.05868e-06\n",
      "i= 7000  x= 8.18312e-07\n",
      "i= 8000  x= 1.10525e-07\n",
      "i= 9000  x= 1.4928e-08\n",
      "final x =  2.02028e-09  y= 4.08155e-18\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(dtype=tf.float32, initial_value=1.0, name='x')\n",
    "y = tf.pow(x, 2)\n",
    "\n",
    "learning_rate = 0.001 \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(y)\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    num_iterations = 10000\n",
    "    for i in xrange(num_iterations):\n",
    "        sess.run(optimizer)\n",
    "        if i % 1000 == 0:\n",
    "            print \"i=\",i,\" x=\", sess.run(x)\n",
    "    print \"final x = \", sess.run(x), \" y=\", sess.run(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it *almost* actually works, right? \n",
    "\n",
    "Yes, the code above works, and this is *exactly* what you should expect from a numerical solver.  It's going to take a very large number of iterations for this alrogithm to reach the exact solution of $x=0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, we have explored the most basic comptation functions of TensorFlow, and we've run a very simple solver to approximate the minimum value of the function $x^2$.  Values to be compted in TensorFlow are stored in **tf.Variable** instances, and the main unit of work is a **computation graph** which is constructed by calling into the core TensorFlow API. \n",
    "\n",
    "We've seen that TensorFlow is a numerical solver, and does **not** produce exact results.  We've also shown that it takes a fairly large number of iterations to get to a final result value, for a trivial example using naive Gradient Descent. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
