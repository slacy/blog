{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Title: Learning to Add\n",
    "Category: TensorFlow From The Ground Up\n",
    "Tags: Python, TensorFlow, Jupyter\n",
    "Date: 2/8/2017 10:00am \n",
    "Author: slacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the **third** post of my series [TensorFlow From The Ground Up]({category}tensorflow-from-the-ground-up).\n",
    "\n",
    "In this post, we'll explore a toy Neural Network, and show our first Perceptron based code.  This is a simple example to get the thought process going, but will lead us to more insights in coming posts. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Machine Learning problems can be thought of like this: \n",
    "\n",
    "* Define a highly-parameterized generic function of the form: $f(inputs) = outputs$\n",
    "* Choose some *Training Data* that are example pairs of $(inputs, outputs)$.\n",
    "* Feed the *Training Data* to an optimizer, which will find values for all the hidden parameters that optimize our *loss function*.\n",
    "* After training, we can use the computed hidden parameters to *validate* example $(input, output)$ pairs that we did not use during training. \n",
    "\n",
    "The power of machine learning is in the design of the computation graph (i.e. \"The Neural Network\") that determines both the parameters, and also determines the flexibility and \"adaptability\" of the resulting model.  In other words:\n",
    "\n",
    "> If we train a model on a collection of input data, does it produce the \n",
    "> expected results for data that it has never seen before?  \n",
    "> \n",
    "> How can we choose a Network Design that we know will solve our target function? \n",
    "\n",
    "The key insight here is understanding your problem area domain, and \"what's possible\" versus \"what's not possible\" in Neural Networks.  We don't yet have this intuition, but we're on our way to building it. \n",
    "\n",
    "Once we have this intuition, it will steer our network design decisions, and can help us to debug any issues we have during the training process.  Additionally, it will guide our expectations about what's possible in the problem domain, and may even lead us into new areas of network and node design.  \n",
    "\n",
    "But as you will see, the idea of a \"Neural Network Node\" is nothing more than an abstract idea that helps us reason about the computation graph.  There is really no such thing as a \"node\" in a network -- there is just a collection of parameters, stored as matrices, and a graph that does computations on these matrices. \n",
    "\n",
    "With that, let's get coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get concrete!\n",
    "\n",
    "Everything above is pretty abstract, so let's start with a concrete problem.  Can we learn the function:\n",
    "\n",
    "$$f(x,y) = x+y$$\n",
    "\n",
    "Using a Neural Network? \n",
    "\n",
    "First, what does it mean to \"learn\" this function?  It will involve the following steps:\n",
    "\n",
    "* Design a computation graph.\n",
    "* Choose some training data, which will be a collection of $(x,y)$ pairs.\n",
    "* Choose some validation data, which will be a collection of $(x,y)$ pairs that were never used during training. \n",
    "* Implement a *Loss function* to determine how correct the values computed are.  This determines the training and accuracy. \n",
    "* Write code to implement the graph, train and validate our problem set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Computation Graph\n",
    "\n",
    "Last time, we talked about TensorFlow ``tf.Variable`` objects, which we used to hold computed values in our graph.  Variables are generally Tensors (i.e. matrices) that hold all the hidden parameters that will be learned by the network. \n",
    "\n",
    "In addition to ``Variables``, we need to pass values to our network.  These values can be thought of the *inputs* to our function.  In TensorFlow, these inputs are instances of the type ``tf.placeholder``.  You can think of a ``placeholder`` as a slot where an input value will go, at runtime.  They aren't *learned parameters* like the ``Variable`` instances are. \n",
    "\n",
    "In general, ``Variable`` and ``placeholder`` instances behave pretty much the same, except that placeholders are *inputs* and Variables are *stored parameters*.  Variables require an initial value, and Placeholders are fed discrete values at runtime with each training iteration. \n",
    "\n",
    "For our learning task, we are going to use a traditional [Perceptron](https://en.wikipedia.org/wiki/Perceptron) based network. Our network will consist of:\n",
    "\n",
    "* One input layer of our $(x,y)$ values.\n",
    "* One middle layer of 2 values, computed using the Perceptron function.\n",
    "* One output value.  \n",
    "\n",
    "Each node in the middle layer will compute \n",
    "\n",
    "$$f(x) = \\sum_{i=0}^n {w_i v_i+b_i}$$\n",
    "\n",
    "The steps of the computation look roughly like this: \n",
    "\n",
    "$$\\begin{array}{rl}\n",
    "inputs & = \\{x,y\\} \\\\\n",
    "l_1 & = w_1 x + w_2 y + b_1 \\\\\n",
    "l_2 & = w_3 x + w_4 y + b_2 \\\\\n",
    "output & = w_5 l_1 + w_6 l_2 + b_3\n",
    "\\end{array}$$\n",
    "\n",
    "If we expand out the temporaries, we get: \n",
    "\n",
    "$$output = w_5 (w_1 x + w_2 y + b_1) + w_6 (w_3 x + w_4 y + b_2) + b_3$$\n",
    "\n",
    "As you can see, we can easily implement $x+y$ by setting: \n",
    "\n",
    "$$\\begin{array}{rl}\n",
    "w_1, w_2, w_5 & = 1.0 \\\\\n",
    "w_3, w_4, w_6 & = 0.0 \\\\\n",
    "b_i & = 0.0 \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "\n",
    "We'll be using TensorFlow's matrix representations to implement these calculations.  All weights and biases shown above are stored as matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "i= 0  loss= [ 343.70489502]\n",
      "i= 1000  loss= [ 0.00222525]\n",
      "i= 2000  loss= [  9.52743212e-05]\n",
      "i= 3000  loss= [  7.60952462e-05]\n",
      "i= 4000  loss= [  2.87974472e-05]\n",
      "i= 5000  loss= [  2.28197314e-06]\n",
      "i= 6000  loss= [  6.43522071e-08]\n",
      "i= 7000  loss= [  3.78495315e-08]\n",
      "i= 8000  loss= [  1.07751148e-05]\n",
      "i= 9000  loss= [  5.25324140e-09]\n",
      "VALIDATE\n",
      "x= -15  y= 29  out= 14.0  loss= [  1.78260962e-10]\n",
      "x= -19  y= -11  out= -29.9999  loss= [  1.88592821e-08]\n",
      "x= 7  y= 9  out= 16.0  loss= [ 0.]\n",
      "x= 3  y= 9  out= 12.0  loss= [  9.09494702e-11]\n",
      "x= -1  y= -35  out= -35.9998  loss= [  2.94676283e-08]\n",
      "x= 21  y= -13  out= 8.00004  loss= [  1.60434865e-09]\n",
      "x= -39  y= 7  out= -31.9999  loss= [  1.63308869e-08]\n",
      "x= -3  y= 17  out= 14.0  loss= [  3.63797881e-12]\n",
      "x= 21  y= 29  out= 49.9999  loss= [  1.14087015e-08]\n",
      "x= -33  y= 3  out= -29.9999  loss= [  1.53704605e-08]\n",
      "[array([[-0.73145592,  0.41055757],\n",
      "       [-0.71037835,  0.44684216]], dtype=float32), array([[ 0.12508716,  0.19557403]], dtype=float32), array([[-1.03099132],\n",
      "       [ 0.59887499]], dtype=float32), array([[ 0.01188867]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import random\n",
    "\n",
    "# Here are the inputs to our computation Graph.  Note that the expected value \n",
    "# f(x,y)=x+y is passed a via a placeholder as well. \n",
    "x = tf.placeholder(shape=(1,), dtype=tf.float32, name='x')\n",
    "y = tf.placeholder(shape=(1,), dtype=tf.float32, name='y')\n",
    "expected = tf.placeholder(shape=(1,), dtype=tf.float32, name='expected')\n",
    "\n",
    "# Inputs need to be in the form of a single tensor, so we concatenate and reshape \n",
    "# into a form that we can use below. \n",
    "inputs = tf.reshape(tf.concat(0, [x,y]), (1,2))\n",
    "\n",
    "# We are constructing a middle layer of 2 perceptron nodes.  Weights and biases \n",
    "# are our w_i and v_i from the above equations. \n",
    "input_weight = tf.Variable(expected_shape=[2,2], \n",
    "                           initial_value=tf.truncated_normal([2,2], mean=0, stddev=0.1))\n",
    "input_bias = tf.Variable(expected_shape=[1, 2], \n",
    "                         initial_value=tf.truncated_normal([1, 2], mean=0, stddev=0.1))\n",
    "\n",
    "# Our middle layer of \"nodes\" (can be thought of as just an intermediate value)\n",
    "# Is an array of size (1,2) and each value is computed from our perceptron function: \n",
    "mid_layer = tf.add(tf.matmul(inputs, input_weight), input_bias)\n",
    "\n",
    "# We're looking for a single output value, so we apply the same perceptron formula \n",
    "# to our middle layer, but this time multiplying by a tensor of size (2,1) to produce \n",
    "# an output of size (1,1).  We then \"squeeze\" this value down to a tensor of size (1)\n",
    "out_weight = tf.Variable(expected_shape=[2,1], \n",
    "                         initial_value=tf.truncated_normal([2,1], mean=0, stddev=0.1))\n",
    "out_bias = tf.Variable(expected_shape=[1, 1], \n",
    "                       initial_value=tf.truncated_normal([1,1], mean=0, stddev=0.1))\n",
    "\n",
    "# Perceptron formula again, and the squeeze to get a single value. \n",
    "output = tf.squeeze(tf.add(tf.matmul(mid_layer, out_weight), out_bias))\n",
    "\n",
    "# Our error function is computed as \"Squared Difference\" between the computed output\n",
    "# and the expected value. \n",
    "loss = tf.pow(output - expected, 2)\n",
    "\n",
    "# Learning rate and optimizer similar to our previous examples. \n",
    "learning_rate = 0.001\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "def f(x,y): return x + y\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run([tf.local_variables_initializer(), \n",
    "              tf.global_variables_initializer()])\n",
    "    \n",
    "    # Training: \n",
    "    #\n",
    "    # Iterate many times with random inputs to \"learn\" the parameters\n",
    "    # stored in input_wegiht, input_bias, out_weight, out_bias Variables above.  \n",
    "    # For this example, we only pass in **even** numbers in the range [0,20]\n",
    "    # \n",
    "    # We will use odd values during our validation phase below to ensure that \n",
    "    # we never validate on any of the inputs that were in the training set. \n",
    "    print \"TRAIN\"\n",
    "    train_iterations = 10000\n",
    "    for i in xrange(train_iterations):\n",
    "        ix = [random.randrange(0, 10) * 2,]\n",
    "        iy = [random.randrange(0, 10) * 2,]\n",
    "        e = [f(ix[0],iy[0]),]\n",
    "        _, l = sess.run([optimizer, loss], feed_dict={x:ix, y:iy, expected:e})\n",
    "        if i % 1000 == 0:\n",
    "            print \"i=\",i,\" loss=\", l\n",
    "            \n",
    "    # Once we have learned the parameters, we can validate by passing inputs \n",
    "    # never seen before.  For this case, we expand the range of our inputs \n",
    "    # to include all odd numbers in the range [-40,40].  \n",
    "    print \"VALIDATE\"\n",
    "    validate_iterations = 10\n",
    "    for i in xrange(validate_iterations):\n",
    "        ix = [random.randrange(-20, 20) * 2 + 1,]\n",
    "        iy = [random.randrange(-20, 20) * 2 + 1,]\n",
    "        e = [f(ix[0], iy[0]),]\n",
    "        out, l = sess.run([output, loss], feed_dict={x:ix, y:iy, expected:e})\n",
    "        print \"x=\",ix[0], \" y=\",iy[0], \" out=\", out, \" loss=\", l\n",
    "    \n",
    "    # Print out the computed weights and bisaes for inspection. \n",
    "    print sess.run([input_weight, input_bias, out_weight, out_bias])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It worked!\n",
    "\n",
    "Looking at the output above, we can see that even when we pass in inputs that are outside the range of the original training data, we produce values for $x+y$ that are approximately within our final loss (i.e. error estimate) value.  \n",
    "\n",
    "**Our network has \"Learned To Add\" just like we thought!**\n",
    "\n",
    "This result isn't a huge surprise, because the perceptron algorithm is a collection of linear functions composed together, and $x+y$ is linear, so it does a great job.  By this same logic, we could easily have this network learn any function that takes the same form as the equation $output$ shown above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we view the parameters? \n",
    "\n",
    "Looking back to the original $output$ equation above, we can see that there are many combinations of weights and biases that will exactly solve our equation.  Which one will the optimizer find?  \n",
    "\n",
    "It turns out that it doesn't really matter, as long as the result is correct.  In fact, it might find other non-obvious combinations of parameters that simplify down to exactly $x+y$.   We can change the code above to print out the values for the weights & biases.  With one run from above, I got the following results: \n",
    "\n",
    "$$\\begin{array}{rl}\n",
    "input\\_weight & = \\begin{bmatrix}-0.83800858 & 0.15946344 \\\\\n",
    "       -0.84973776 & 0.08742908\\end{bmatrix} \\\\\n",
    "input\\_bias & = \\begin{bmatrix}0.13537928 & -0.1656988\\end{bmatrix} \\\\\n",
    "out\\_weight & = \\begin{bmatrix}-1.15744114 & 0.18846205\\end{bmatrix} \\\\\n",
    "out\\_bias & = 0.18807185\n",
    "\\end{array}$$\n",
    "\n",
    "I'll just let you trust me that if you pass these values into the graph above that the result approximates $x+y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## You should play with this code a little bit.\n",
    "\n",
    "Here's a collection of random ideas for how to play around with the code example above and gain some insights:\n",
    "\n",
    "* Modify the \"f()\" function to try other linear combinations of x & y.  Can it learn $x-y$?  Can it learn $0.5x + 0.75y - 0.33$? \n",
    "* Modify the size of the middle layer.  We use 2 middle layer nodes.  What if you use 200?  How does that impact learning rate?   \n",
    "* Modify the size of the middle layer, and have it try to learn something \"Hard\" like $x\\cdot y$. Did it work?  Do you have any thoughts about why or why not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
