{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Title: A Word About Batching\n",
    "Category: TensorFlow From The Ground Up\n",
    "Tags: Python, TensorFlow, Jupyter\n",
    "Date: 2/9/2017 1:10pm\n",
    "Author: slacy\n",
    "Status: draft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an **interlude** from my series [TensorFlow From The Ground Up]({category}tensorflow-from-the-ground-up).\n",
    "\n",
    "In a previous post, [Learning to Add]({filename}Learning to Add.md) I showed simple code to run & train a neural network.  That code example trains \"one example at a time\".  This is generally *not* how you implement a typical large ML training system. \n",
    "\n",
    "Its much more typical to train in **batches**.  Batches are a way to pass a medium to large collection of training examples to our network, run the expensive calculations just once on the entire batch, then update the gradients and train on the next batch.   Because everything in TensorFlow is a *tensor* (a multi-dimensional array), the process of batching is nothing more than adding an extra dimension to all the tensors used in the system.  In fact, there are several parts of TensorFlow that assume that the first dimension of a tensor is the size of a batch.  Here is an example using an unknown batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of intermediate = (?, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# This declares a batch of unknown size, of tensors of size 4. \n",
    "ins = tf.placeholder(dtype=tf.float32, shape=(None, 4))\n",
    "\n",
    "# Our weights and biases are declared as usual.  The batch size does \n",
    "# not effect their declaration. \n",
    "w = tf.Variable(\n",
    "    expected_shape=(4, 3), dtype=tf.float32, \n",
    "    initial_value=tf.ones(shape=(4, 3)))\n",
    "b = tf.Variable(\n",
    "    expected_shape=(3,), dtype=tf.float32, \n",
    "    initial_value=tf.ones(shape=(3,)))\n",
    "\n",
    "# We compute \"out = ins * w + b\" without worrying what the first dimension of \n",
    "# these tensors are. \n",
    "out = tf.add(tf.matmul(ins, w), b)\n",
    "\n",
    "# When we print the shape of the final output, it has an unknown \n",
    "# size for the first dimension as well. \n",
    "print \"Shape of intermediate =\", out.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional benefits of batching\n",
    "\n",
    "Batching our training data can actually make training faster as well, and not just because the computation is more efficient.  Remember that we're training via *GradientDescent* (and similar algorithms).  These algorithms look at the current set of inputs, and decide how to tweak internally stored parameters to minimize the *loss function*.  \n",
    "\n",
    "But, there are lots of cases where optimizing one training example has the opposite effect on other training examples.  These examples can tend to \"See-saw\" back and forth between two not very optimial sets of parameters during training.  You can think of each example as *pulling* the parameters in opposite directions.\n",
    "\n",
    "When we batch, we give multiple training examples to the optimizer.  If two examples pull parmeters in opposite directions, then they will end up cancelling each other out, and other solutions will be tried.\n",
    "\n",
    "There are several library functions within TensorFlow itself to facilitate batching, but here, I'm going to be doing the work myself, to make it more clear how batching really works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "The example that I'm going to use an example is a 4-bit parity problem.  My input is going to be a Tensor of shape (4), with each value being either 0.0 or 1.0.  Expressed as a mathematical function, what I'd like to compute is: \n",
    "\n",
    "$$ f(x_i)= \\sum_{i=0}^n{x_i} \\bmod{2} $$\n",
    "\n",
    "The network we'll use will be a one-layer network with 16 nodes in the middle layer. \n",
    "\n",
    "So, here we go: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "i= 0  loss= [ 343.70489502]\n",
      "i= 1000  loss= [ 0.00222525]\n",
      "i= 2000  loss= [  9.52743212e-05]\n",
      "i= 3000  loss= [  7.60952462e-05]\n",
      "i= 4000  loss= [  2.87974472e-05]\n",
      "i= 5000  loss= [  2.28197314e-06]\n",
      "i= 6000  loss= [  6.43522071e-08]\n",
      "i= 7000  loss= [  3.78495315e-08]\n",
      "i= 8000  loss= [  1.07751148e-05]\n",
      "i= 9000  loss= [  5.25324140e-09]\n",
      "VALIDATE\n",
      "x= -15  y= 29  out= 14.0  loss= [  1.78260962e-10]\n",
      "x= -19  y= -11  out= -29.9999  loss= [  1.88592821e-08]\n",
      "x= 7  y= 9  out= 16.0  loss= [ 0.]\n",
      "x= 3  y= 9  out= 12.0  loss= [  9.09494702e-11]\n",
      "x= -1  y= -35  out= -35.9998  loss= [  2.94676283e-08]\n",
      "x= 21  y= -13  out= 8.00004  loss= [  1.60434865e-09]\n",
      "x= -39  y= 7  out= -31.9999  loss= [  1.63308869e-08]\n",
      "x= -3  y= 17  out= 14.0  loss= [  3.63797881e-12]\n",
      "x= 21  y= 29  out= 49.9999  loss= [  1.14087015e-08]\n",
      "x= -33  y= 3  out= -29.9999  loss= [  1.53704605e-08]\n",
      "[array([[-0.73145592,  0.41055757],\n",
      "       [-0.71037835,  0.44684216]], dtype=float32), array([[ 0.12508716,  0.19557403]], dtype=float32), array([[-1.03099132],\n",
      "       [ 0.59887499]], dtype=float32), array([[ 0.01188867]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import random\n",
    "\n",
    "inputs = tf.placeholder(shape=(4,), dtype=tf.float32, name='y')\n",
    "expected = tf.placeholder(shape=(1,), dtype=tf.float32, name='expected')\n",
    "\n",
    "# We are constructing a middle layer of 2 perceptron nodes.  Weights and biases \n",
    "# are our w_i and v_i from the above equations. \n",
    "input_weight = tf.Variable(expected_shape=(4, 16), \n",
    "                           initial_value=tf.truncated_normal((4, 16), mean=0, stddev=0.1))\n",
    "input_bias = tf.Variable(expected_shape=(1, 16), \n",
    "                         initial_value=tf.truncated_normal((1, 16), mean=0, stddev=0.1))\n",
    "\n",
    "mid_layer = tf.nn.sigmoid(tf.add(tf.matmul(inputs, input_weight), input_bias))\n",
    "\n",
    "out_weight = tf.Variable(expected_shape=(16, 1), \n",
    "                         initial_value=tf.truncated_normal((16, 1), mean=0, stddev=0.1))\n",
    "out_bias = tf.Variable(expected_shape=(1, 1), \n",
    "                       initial_value=tf.truncated_normal((1, 1), mean=0, stddev=0.1))\n",
    "\n",
    "# Perceptron formula again, and the squeeze to get a single value. \n",
    "output = tf.squeeze(tf.add(tf.matmul(mid_layer, out_weight), out_bias))\n",
    "\n",
    "# Our error function is computed as \"Squared Difference\" between the computed output\n",
    "# and the expected value. \n",
    "loss = tf.pow(output - expected, 2)\n",
    "\n",
    "# Learning rate and optimizer similar to our previous examples. \n",
    "learning_rate = 0.001\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "def f(x,y): return x + y\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run([tf.local_variables_initializer(), \n",
    "              tf.global_variables_initializer()])\n",
    "    \n",
    "    # Training: \n",
    "    #\n",
    "    # Iterate many times with random inputs to \"learn\" the parameters\n",
    "    # stored in input_wegiht, input_bias, out_weight, out_bias Variables above.  \n",
    "    # For this example, we only pass in **even** numbers in the range [0,20]\n",
    "    # \n",
    "    # We will use odd values during our validation phase below to ensure that \n",
    "    # we never validate on any of the inputs that were in the training set. \n",
    "    print \"TRAIN\"\n",
    "    train_iterations = 10000\n",
    "    for i in xrange(train_iterations):\n",
    "        ix = [random.randrange(0, 10) * 2,]\n",
    "        iy = [random.randrange(0, 10) * 2,]\n",
    "        e = [f(ix[0],iy[0]),]\n",
    "        _, l = sess.run([optimizer, loss], feed_dict={x:ix, y:iy, expected:e})\n",
    "        if i % 1000 == 0:\n",
    "            print \"i=\",i,\" loss=\", l\n",
    "            \n",
    "    # Once we have learned the parameters, we can validate by passing inputs \n",
    "    # never seen before.  For this case, we expand the range of our inputs \n",
    "    # to include all odd numbers in the range [-40,40].  \n",
    "    print \"VALIDATE\"\n",
    "    validate_iterations = 10\n",
    "    for i in xrange(validate_iterations):\n",
    "        ix = [random.randrange(-20, 20) * 2 + 1,]\n",
    "        iy = [random.randrange(-20, 20) * 2 + 1,]\n",
    "        e = [f(ix[0], iy[0]),]\n",
    "        out, l = sess.run([output, loss], feed_dict={x:ix, y:iy, expected:e})\n",
    "        print \"x=\",ix[0], \" y=\",iy[0], \" out=\", out, \" loss=\", l\n",
    "    \n",
    "    # Print out the computed weights and bisaes for inspection. \n",
    "    print sess.run([input_weight, input_bias, out_weight, out_bias])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## You should play with this code a little bit.\n",
    "\n",
    "Here's a collection of random ideas for how to play around with the code example above and gain some insights:\n",
    "\n",
    "* Modify the \"f()\" function to try other linear combinations of x & y.  Can it learn $x-y$?  Can it learn $0.5x + 0.75y - 0.33$? \n",
    "* Modify the size of the middle layer.  We use 2 middle layer nodes.  What if you use 200?  How does that impact learning rate?   \n",
    "* Modify the size of the middle layer, and have it try to learn something \"Hard\" like $x\\cdot y$. Did it work?  Do you have any thoughts about why or why not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
